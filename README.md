# Reasoning Worse Than Base

A research project investigating why thinking/reasoning models sometimes perform worse than their base (non-reasoning) counterparts.

## Motivation

Large Language Models with explicit reasoning capabilities (e.g., chain-of-thought, extended thinking) are expected to outperform base models on complex tasks. However, empirical evidence shows that reasoning models can underperform base models in certain scenarios. This project aims to systematically study and characterize these failure modes.

## Research Questions

- **When** does reasoning hurt? What task types, domains, or conditions trigger degradation?
- **Why** does it happen? What mechanisms in the reasoning process lead to worse outputs?
- **How** can it be mitigated? Can we predict or prevent reasoning-induced performance drops?

## Project Structure

```
├── data/           # Datasets and evaluation results
├── experiments/    # Experiment scripts and configurations
├── analysis/       # Analysis notebooks and scripts
├── paper/          # Paper drafts and figures
└── README.md
```

## Getting Started

_Coming soon._

## License

MIT
